# -*- coding: utf-8 -*-
"""Breast Cancer Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16dMYcbPXpM1isXOXtOQrSi9hYDwCHqGk
"""

#import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#data load
from google.colab import files
upload = files.upload()
df = pd.read_csv('data.csv')
df.head(7)

#rows and columns in data set
df.shape

#count of empty value in each column
df.isna().sum()

#drop column unnamed:32
df = df.dropna(axis=1)

#get new count of number of rows and columns
df.shape

#count of Malognant or Benign M or B
df['diagnosis'].value_counts()

#count viz
sns.countplot(df['diagnosis'], label='count')

#data types that need to be encoded
df.dtypes

#encode categoriacal data values m =1 b =0
from sklearn.preprocessing import LabelEncoder
LabelEncoder_Y = LabelEncoder()
df.iloc[:,1] = LabelEncoder_Y.fit_transform(df.iloc[:,1].values)

#B = blue M = orange
image1 = sns.pairplot(df.iloc[:,1:6], hue = 'diagnosis')
image1.figure.savefig("BCDiagnosisMB.png")

df.head(5)

df.iloc[:,1:12].corr()

plt.figure(figsize=(10,10))
image2 = sns.heatmap(df.iloc[:,1:12].corr(), annot=True, fmt='.0%')
image2.figure.savefig("heatmapcorrelationpercentageBC.png")

#split data set into independant = x and dependant = y data sets
X = df.iloc[:,2:31].values
Y = df.iloc[:,1].values

#split data 75% training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

#Scale the data (feature scaling)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

#function for models
def models(X_train, Y_train):

 #Logistics regression will be the first model 
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  #Decision Tree(criterion=entropy, expresses information gain)
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion="entropy", random_state = 0)
  tree.fit(X_train, Y_train)

  #Random forest classifier 
  from sklearn.ensemble import RandomForestClassifier 
  forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)

  #Print models accuracy on the training data
  print('[0]Logistic Regression Training accuracy:', log.score(X_train, Y_train))
  print('[1]Decision Tree Classifier Training accuracy:', tree.score(X_train, Y_train))
  print('[2]Random Forest Classifier Training accuracy:', forest.score(X_train, Y_train))
  
  return log, tree, forest

#retreiving all models
model = models(X_train, Y_train)

#test model accuracy on test data on confusion matrix
from sklearn.metrics import confusion_matrix 

for i in range(len(model)):
  print('Model', i)
  cm = confusion_matrix(Y_test, model[i].predict(X_test))

  #TP(True Positive) TN(True Negative) FN(False Negative) FP(False Positive)
  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]

  print(cm)
  print('Testing Accuracy = ',(TP+TN)/(TP+TN+FN+FP))
  print()

#other ways to get metrics of models
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model', i)
  print(classification_report(Y_test, model[i].predict(X_test)))
  print(accuracy_score(Y_test, model[i].predict(X_test)))
  print()

#Random Forest Prediction model
pred = model[2].predict(X_test)
print(pred)
print()
print(Y_test)

